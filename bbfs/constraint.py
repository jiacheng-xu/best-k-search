from transformers import Constraint
from typing import List

# modified version of the original PhrasalConstraint. 
# reset: self.fulfilled_idx = 0
# => reset: self.fulfilled_idx = -1

class CustomPhrasalConstraint(Constraint):
    r"""
    [`Constraint`] enforcing that an ordered sequence of tokens is included in the output.

    Args:
        token_ids (`List[int]`):
            The id of the token that must be generated by the output.
    """

    def __init__(self, token_ids: List[int]):
        super(Constraint, self).__init__()

        if not isinstance(token_ids, list) or len(token_ids) == 0:
            raise ValueError(f"`token_ids` has to be a non-empty list, but is {token_ids}.")
        if any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids):
            raise ValueError(f"Each list in `token_ids` has to be a list of positive integers, but is {token_ids}.")

        self.token_ids = token_ids

        self.seqlen = len(self.token_ids)
        self.fulfilled_idx = -1  # the index of the currently fulfilled step
        self.completed = False

    def advance(self):
        if self.completed:
            return None
        return self.token_ids[self.fulfilled_idx + 1]

    def does_advance(self, token_id: int):
        if not isinstance(token_id, int):
            raise ValueError(f"`token_id` has to be an `int`, but is {token_id} of type {type(token_id)}")

        if self.completed:
            return False

        return token_id == self.token_ids[self.fulfilled_idx + 1]

    def update(self, token_id: int):
        if not isinstance(token_id, int):
            raise ValueError(f"`token_id` has to be an `int`, but is {token_id} of type {type(token_id)}")

        stepped = False
        completed = False
        reset = False

        if self.does_advance(token_id):
            self.fulfilled_idx += 1
            stepped = True
            if self.fulfilled_idx == (self.seqlen - 1):
                completed = True
            self.completed = completed
        else:
            # failed to make progress.
            reset = True
            self.reset()
        return stepped, completed, reset

    def reset(self):
        self.completed = False
        self.fulfilled_idx = -1

    def remaining(self):
        return self.seqlen - (self.fulfilled_idx + 1)

    def copy(self, stateful=False):
        new_constraint = CustomPhrasalConstraint(self.token_ids)

        if stateful:
            new_constraint.seq_len = self.seqlen
            new_constraint.fulfilled_idx = self.fulfilled_idx
            new_constraint.completed = self.completed

        return new_constraint
"""
constraint_token = [22, 44 , 66]
const = CustomPhrasalConstraint(constraint_token)
print(const.update(40))
print(const.update(22))
print(const.update(44))
print(const.update(11))
print(const.update(66))
print(const.update(22))
print(const.update(44))
stepped, completed, reset = const.update(66)
print(stepped, completed, reset)
print(const.update(22))
print(const.update(44))
"""
from transformers import (
        AutoTokenizer,
        AutoModelForSeq2SeqLM,
        LogitsProcessorList,
        MinLengthLogitsProcessor,
        ConstrainedBeamSearchScorer,
        PhrasalConstraint,
    )


import torch
class ConstraintBeamSearch:
    def __init__(self, model, tokenizer, device, max_len,  beam_size, task_rwd, do_sample=False, typical_p=1., top_p=1.0, num_beam_groups=1,diversity_penalty=0.0, verbose=True) -> None:
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.max_len = max_len
        self.beam_size = beam_size
        self.verbose = verbose
        self.min_tokens_to_keep = beam_size
        self.filter_value = -float("Inf")
        self.do_sample = do_sample
        self.typical_p = typical_p
        self.top_p = top_p
        self.num_beam_groups = num_beam_groups
        self.diversity_penalty = diversity_penalty
        self.task_rwd = task_rwd
    def run():

        encoder_input_str = "translate English to German: How old are you?"
        encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


        # lets run beam search using 3 beams
        num_beams = 3
        # define decoder start token ids
        input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
        input_ids = input_ids * model.config.decoder_start_token_id

        # add encoder_outputs to model keyword arguments
        model_kwargs = {
            "encoder_outputs": model.get_encoder()(
                encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
            )
        }

        constraint_str = "Sie"
        constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
        constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


        # instantiate beam scorer
        beam_scorer = ConstrainedBeamSearchScorer(
            batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
        )

        # instantiate logits processors
        logits_processor = LogitsProcessorList(
            [
                MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
            ]
        )

        outputs = model.constrained_beam_search(
            input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
        )

        tokenizer.batch_decode(outputs, skip_special_tokens=True)