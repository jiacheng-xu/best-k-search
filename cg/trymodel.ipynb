{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:1116: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-common_gen\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-common_gen\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a teddy bear runs over as the boys team go into a field of',\n",
       " 'The football team is running drills on the field.',\n",
       " 'A man runs the drill in an open field as his team prepares for a match.',\n",
       " 'The men are running along an artificial grass field on their training team.',\n",
       " 'A group of team members are running drills through the field.',\n",
       " 'a team is running drill in a field',\n",
       " 'A soccer team runs down a field during drills.',\n",
       " 'A team runs through a field of clay during drills.',\n",
       " 'A team runs on a field during drills.',\n",
       " 'A team of people are running drills in a field.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"team run drill field\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# generate up to 30 tokens\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=20, top_p=0.95,num_return_sequences=10,num_beams=1)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.',\n",
       " 'A man sitting on a couch blowing balloons.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gen_sentence(words, max_length=20, beam_size= 10):\n",
    "    input_text = words\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length, num_beams=beam_size, do_sample=True, top_p=0.9, num_return_sequences=beam_size)\n",
    "\n",
    "    return [tokenizer.decode(output[i], skip_special_tokens=True) for i in range(len(output))]\n",
    "\n",
    "\n",
    "# output: digging a hole in the ground to plant trees\n",
    "\n",
    "words = \"tree plant ground hole dig\"\n",
    "words = \"ride wear hat horse\"\n",
    "words = \" \".join(['balloon', 'couch', 'blow', 'sit'])\n",
    "# words =' '.join(['snow', 'stand', 'dog', 'look']) \n",
    "# words = ' '.join(['front', 'trick', 'perform', 'crowd'])\n",
    "gen_sentence(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/anaconda3/lib/python3.9/site-packages/transformers/generation_beam_search.py:196: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect.`max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`,or `group_beam_search(...)`.\n",
      "  warnings.warn(\n",
      "/export/home/anaconda3/lib/python3.9/site-packages/transformers/generation_utils.py:2451: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "/export/home/anaconda3/lib/python3.9/site-packages/transformers/generation_utils.py:2474: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_beams * (beam_idx // group_size) + group_start_idx + (beam_idx % group_size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A man wearing a hat rides a horse.',\n",
       " 'A man riding a horse in a hat.',\n",
       " 'A man wearing a hat rides a horse..',\n",
       " 'A young girl wearing a hat rides a horse.',\n",
       " 'a man wears a hat while riding his horse',\n",
       " 'A man wearing a hat rides a horse.',\n",
       " 'A man riding a horse wearing a hat.',\n",
       " 'A man riding a horse in a hat..',\n",
       " 'A man is riding a horse wearing a hat.',\n",
       " 'A woman is riding a horse wearing a hat.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gen_sentence(words, max_length=20, beam_size= 10):\n",
    "    input_text = words\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length, num_beams=beam_size, num_beam_groups=5,diversity_penalty=0.5, num_return_sequences=beam_size)\n",
    "\n",
    "    return [tokenizer.decode(output[i], skip_special_tokens=True) for i in range(len(output))]\n",
    "\n",
    "\n",
    "# output: digging a hole in the ground to plant trees\n",
    "\n",
    "words = \"tree plant ground hole dig\"\n",
    "words = \"ride wear hat horse\"\n",
    "# words =' '.join(['snow', 'stand', 'dog', 'look']) \n",
    "# words = ' '.join(['front', 'trick', 'perform', 'crowd'])\n",
    "gen_sentence(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def distinctness(outputs, EOT_TOKEN='.'):\n",
    "    cw = \"default\"\n",
    "    d1, d2, d3 = defaultdict(lambda: set()), defaultdict(lambda: set()), defaultdict(lambda: set())\n",
    "    total_words = defaultdict(lambda: 0)\n",
    "\n",
    "    for o in outputs:\n",
    "        o = o.replace(EOT_TOKEN, ' ').strip().split(' ')\n",
    "        o = [str(x) for x in o]\n",
    "        total_words[cw] += len(o)\n",
    "        d1[cw].update(o)\n",
    "        for i in range(len(o) - 1):\n",
    "            d2[cw].add(o[i] + ' ' + o[i+1])\n",
    "        for i in range(len(o) - 2):\n",
    "            d3[cw].add(o[i] + ' ' + o[i+1] + ' ' + o[i+2])\n",
    "    return_info = []\n",
    "    avg_d1, avg_d2, avg_d3 = 0, 0, 0\n",
    "    for cw in total_words.keys():\n",
    "        return_info.append((cw, 'DISTINCTNESS', len(d1[cw]) / total_words[cw], len(d2[cw]) / total_words[cw], len(d3[cw]) / total_words[cw]))\n",
    "        avg_d1 += len(d1[cw]) / total_words[cw]\n",
    "        avg_d2 += len(d2[cw]) / total_words[cw]\n",
    "        avg_d3 += len(d3[cw]) / total_words[cw]\n",
    "    avg_d1, avg_d2, avg_d3 = avg_d1 / len(total_words.keys()), avg_d2 / len(total_words.keys()), avg_d3 / len(total_words.keys())\n",
    "    return return_info, (avg_d1, avg_d2, avg_d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('default',\n",
       "   'DISTINCTNESS',\n",
       "   0.5217391304347826,\n",
       "   0.5652173913043478,\n",
       "   0.4782608695652174)],\n",
       " (0.5217391304347826, 0.5652173913043478, 0.4782608695652174))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinctness([\"this is a sentence\",\"this is another\",\"this is another\",\"this is another\",\"asdas asd sd s ds ds dasd asd asd as \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration alexfabbri--answersumm-fc26db571c9b9cb8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/alexfabbri--answersumm to /root/.cache/huggingface/datasets/alexfabbri___json/alexfabbri--answersumm-fc26db571c9b9cb8/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37895e65e82042a7ac0cf4c0ec7b8240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9935b285a184f49adaa3a335969ef91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e996cc26c85745e1a74240ebc07a9ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc8c145609b4820ba5bedf429c2972e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eb79d612b340729eaf20ed62031a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39eaac8a774e490a918e29bea5254054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f3ac8065342e9895310161f38264d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499ecc41511343bdbad382400e0b3ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/alexfabbri___json/alexfabbri--answersumm-fc26db571c9b9cb8/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f068a0eeda423988e0cb28dbcc7173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "answersumm = load_dataset(\"alexfabbri/answersumm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "{'answers': [{'sents': [{'text': \"As a hiring manager I've asked questions like this in the past and from my perspective\", 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'I want to hear a story where you had to use your brain to decide on a course of action and how you implemented it.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Was the decision technically sound?', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Did you know the path to success, and were you confident?', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': \"Were there inter-personal, inter-department, or other 'fuzzy' issues that could have prevented it from being successful?\", 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Can you show me some passion that you were responsible for it?', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': \"If you can't think of anything technical that closely relates to the job description, then tell me any story where you made a decision and ran with it.\", 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'I can really get a lot out of this answer ...', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'How you deal with uncertainty, are you motivated to solve problems, can you communicate clearly to me, can you hold my attention from start to finish, do you have the technical and emotional chops to survive in my place of employment, and a few others.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}], 'answer_details': {'author': 'workplace.stackexchange.com/users/84055/Jim Horn', 'score': 20}}, {'sents': [{'text': 'This technique is called \"behavioral interviewing.\"', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': \"By asking about a time you actually did something, ideally the interviewer can get a better sense of how you do things than just asking a hypothetical - real life is complex, and they want to know how you deal with real decisions in real environments, where people disagree for good and bad reasons, where the best technical option isn't always the feasible business option...\", 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'The best answer to a question like this does showcase your success, but also goes into details about how you overcame issues and conflict (ideally without coming across like a rabid wolverine).', 'label': [1], 'label_summ': [1], 'cluster_id': [[0]]}, {'text': 'There are plenty of people whose examples provide a questionable thought process - \"I wanted to use it because it was new even though it might not have been a good fit/', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'I wanted to use it even though it was old because I\\'m familiar with it\", \"I just went ahead and did it even though others objected because screw them', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': '/', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'I gave in to what the lead wanted because who cares anyway\"...', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'You want to project confidence, but confidence that is warranted because you understood the technical and organizational impacts of that choice, you can successfully negotiate technical decisions with others, and you can show good judgement and select implementations that will be successful.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}], 'answer_details': {'author': 'workplace.stackexchange.com/users/16695/mxyzplk - SE stop being evil', 'score': 7}}, {'sents': [{'text': 'Anyone seasoned in the industry will have hundred of such stories.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'It is a fairly easy question to answer, and it is not a trick question.', 'label': [1], 'label_summ': [1], 'cluster_id': [[1]]}, {'text': 'For me it is much a more saner question than filler BS questions as \"were you see yourself in 5 years...\" or \"what are your strengths\".', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'I will take pleasure in answer such a question with a couple of examples.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'In reality, I just have to ask how much time they want me to spend on that question, running the risk of taking too long.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Those kind of questions are mostly to gauge your maturity, line of experience and thinking.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Even when around new work mates, we usually share these kind of war stories over a cup of coffee.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': '(cultural note, here we tend more to gather together over coffee than beer)', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}], 'answer_details': {'author': 'workplace.stackexchange.com/users/44976/Rui F Ribeiro', 'score': 5}}, {'sents': [{'text': 'Is this a kind of a \"trick\" question to see if one jumps into giving a technical answer?', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'How should one give a proper answer to this?', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Not really.', 'label': [1], 'label_summ': [0], 'cluster_id': [[1]]}, {'text': 'When this -very common, IMHO-', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'question comes around the idea is to get a feel of what kind of things matter to you when making a complex decision, and to see which points do you prioritise.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': \"It is true that the technical level at which you give your answer matters (as with experience you should get better at understanding what is the role in the organisation of the person that it is in front of you - and that's a valuable communication skill), but it shouldn't be the most important part\", 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'Instead, the \"trick\" is more on showing that you have been in a relatively complex situation in the past, where you have been considering multiple options , and where you finally ended up making a sound decision .', 'label': [1], 'label_summ': [1], 'cluster_id': [[0]]}, {'text': 'An answer with something like that should be more than adequate.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}], 'answer_details': {'author': 'workplace.stackexchange.com/users/69905/carrdelling', 'score': 4}}, {'sents': [{'text': 'A question like this can also help to weed out candidates who are just pretending.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'When people make technical decisions, if they researched the decisions and implemented them, they usually end up knowing the tiny details as to why the decision was made, how it was implemented and how any issues related to this decision were resolved.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}, {'text': 'With people who pretend to know, they will try to provide very little detail and will usually get stuck when the interviewer begins to pry.', 'label': [0], 'label_summ': [0], 'cluster_id': [[-1]]}], 'answer_details': {'author': 'workplace.stackexchange.com/users/52839/JJohnston', 'score': 4}}], 'question': {'question': 'What is a good way to answer a question such as \"Tell us about a technical decision you had to make in the past\" during an interview for a software development job?   Is this a trick question to see if the interviewee jumps into giving a technical answer? What would be a proper answer to this?', 'title': 'Answering \"Tell us about a technical decision you had to make in the past\"', 'forum': 'workplace.stackexchange.com', 'question_tags': '<interviewing><software-industry>', 'link': 'workplace.stackexchange.com/questions/111176', 'author': 'workplace.stackexchange.com/users/82795/birdybird03'}, 'example_id': '0_30', 'summaries': [['This is a question about overcome issues and conflict to make a sound decision. It is not a trick question.', 'This sort of question is not generally considered to be a trick question. The most appropriate answer would be simply to describe how you approached a past problem, what solutions you identified and how you chose the one to pursue.']], 'mismatch_info': {'rel_sent_not_in_cluster': [False], 'cluster_sents_not_matched': [[]]}, 'annotator_id': [0], 'cluster_summaries': [['The best answer to this question would be to simply explain how your approached the problem, what potential solutions you identified and how you chose the one to pursue.', 'This is not generall considered a trick question.']]}\n"
     ]
    }
   ],
   "source": [
    "dev = answersumm['validation']\n",
    "print(len(dev))\n",
    "print(dev[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c4026a6dfc46e3b41675ae7da4b487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8226eb88fe54435881546ef34dbd91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f987b367bbc41ba99115408c03ba50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed773bfd41c54be388c6b52413ee6468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Unbabel/gec-t5_small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like swimming and walking to the classroom. However, today is a different day for me.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = \"I like to swimming and walking to the classrom. Hoowever, today is a different day for me.\"\n",
    "sentence = \"Also the work of the media , television , magazines , and every stuff made for publicity affects the emotions of many people around the world , encouraging them to buy and buy anything they can .\"\n",
    "tokenized_sentence = tokenizer('gec: ' + sentence, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "corrected_sentence = tokenizer.decode(\n",
    "    model.generate(\n",
    "        input_ids = tokenized_sentence.input_ids,\n",
    "        attention_mask = tokenized_sentence.attention_mask, \n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "    )[0],\n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(corrected_sentence) # -> I like swimming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79625/2172818720.py:17: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  mauve = load_metric('mauve')\n",
      "[nltk_data] Downloading package wordnet to /export/home/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /export/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /export/home/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import random\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSequenceClassification\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "mauve = load_metric('mauve')\n",
    "rouge = load_metric('rouge')\n",
    "meteor = load_metric( 'meteor')\n",
    "bleu = load_metric('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (1) and references (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/export/home/cond-text-gen/cg/trymodel.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/export/home/cond-text-gen/cg/trymodel.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mIt is a guide to action which ensures that the military always obeys the commands of the party\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/export/home/cond-text-gen/cg/trymodel.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m references \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mIt is a guide to action that ensures that the military will forever heed Party commands\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mIt is a guide to action that ensures that the military will forever heed Party commands\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/export/home/cond-text-gen/cg/trymodel.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m results \u001b[39m=\u001b[39m meteor\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mreferences)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/export/home/cond-text-gen/cg/trymodel.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/metric.py:442\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/metric.py:519\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    514\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(references)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m     )\n\u001b[0;32m--> 519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (1) and references (2)"
     ]
    }
   ],
   "source": [
    "predictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\n",
    "references = [\"It is a guide to action that ensures that the military will forever heed Party commands\",\"It is a guide to action that ensures that the military will forever heed Party commands\"]\n",
    "results = meteor.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "from spacy.pipeline.tagger import DEFAULT_TAGGER_MODEL\n",
    "config = {\"model\": DEFAULT_TAGGER_MODEL}\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "    new_sentence_list = []\n",
    "    for sent in sentences:\n",
    "        a = ''\n",
    "        for token in nlp(sent):\n",
    "            print(token.text)\n",
    "            a += token.text\n",
    "            a += ' '\n",
    "\n",
    "        new_sentence_list.append(a.rstrip())\n",
    "    return new_sentence_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "  \n",
      "sentence\n",
      "?\n",
      "ËøôÊòØ‰∏ÄÂè•‰∏≠Êñá\n",
      "„ÄÇ\n",
      "['This is a    sentence ?', 'ËøôÊòØ‰∏ÄÂè•‰∏≠Êñá „ÄÇ']\n"
     ]
    }
   ],
   "source": [
    "out = tokenize(['This is a   sentence? ','ËøôÊòØ‰∏ÄÂè•‰∏≠Êñá„ÄÇ'])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an English sentence.\n",
      "this PRON nsubj\n",
      "is AUX ROOT\n",
      "an DET det\n",
      "English ADJ amod\n",
      "sentence NOUN attr\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Im vergangenen Jahr versch√§rfte sich die Lage weiter, als das weltgr√∂√üte Erzeugerland China angesichts der schw√§chelnden heimischen Nachfrage massenhaft Stahl auf den Weltmarkt warf.']\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "sentences = ['this is an English sentence.']\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
