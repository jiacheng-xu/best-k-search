{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copyreg import pickle\n",
    "from operator import le\n",
    "from readline import parse_and_bind\n",
    "import torch\n",
    "from transformers import AutoConfig,AutoModelForSeq2SeqLM,AutoTokenizer\n",
    "from dataclasses import dataclass,field \n",
    "from typing import List\n",
    "import math\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_model(task='sum', dataset='xsum', model_name='facebook/bart-large-xsum', device_name='cuda:0'):\n",
    "    device = torch.device(device_name)\n",
    "    print(model_name)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if task == 'custom':\n",
    "        # you need to store the input under the path_dataset folder\n",
    "        dec_prefix = [tokenizer.eos_token_id]\n",
    "        with open(os.path.join(dataset, 'input.txt'), 'r') as fd:\n",
    "            slines = fd.read().splitlines()\n",
    "        with open(os.path.join(dataset, 'output.txt'), 'r') as fd:\n",
    "            tlines = fd.read().splitlines()\n",
    "        dataset = zip(slines, tlines)\n",
    "    elif task == 'sum':\n",
    "        logging.info('Loading dataset')\n",
    "        if dataset == 'xsum':\n",
    "            dataset = load_dataset(\"xsum\", split='validation')\n",
    "        elif dataset == 'cnndm':\n",
    "            raise NotImplementedError(\"not supported\")\n",
    "            dataset = load_dataset(\"cnn_dailymail\", split='validation')\n",
    "            print(\"CNNDM mean token in ref 56\")\n",
    "        dec_prefix = [tokenizer.eos_token_id]\n",
    "    elif task == 'mt1n':\n",
    "        from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "        model = MBartForConditionalGeneration.from_pretrained(\n",
    "            \"facebook/mbart-large-50-one-to-many-mmt\")\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "            \"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n",
    "        assert dataset.startswith('en')\n",
    "        tgt_lang = dataset[3:]\n",
    "        dataset = read_mt_data(name=dataset)\n",
    "\n",
    "        from transformers.models.mbart.tokenization_mbart import FAIRSEQ_LANGUAGE_CODES\n",
    "        match = [x for x in FAIRSEQ_LANGUAGE_CODES if x.startswith(tgt_lang)]\n",
    "        assert len(match) == 1\n",
    "        lang = match[0]\n",
    "        logging.info(f\"Lang: {lang}\")\n",
    "        dec_prefix = [tokenizer.eos_token_id, tokenizer.lang_code_to_id[lang]]\n",
    "        logging.info(f\"{tokenizer.decode(dec_prefix)}\")\n",
    "    elif task == 'mtn1':\n",
    "        from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "        model = MBartForConditionalGeneration.from_pretrained(\n",
    "            \"facebook/mbart-large-50-many-to-one-mmt\", )\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "            \"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "        # dataset should be like \"xx-en\"\n",
    "        assert dataset.endswith('-en')\n",
    "        src_lang = dataset[:2]\n",
    "        from transformers.models.mbart.tokenization_mbart import FAIRSEQ_LANGUAGE_CODES\n",
    "        match = [x for x in FAIRSEQ_LANGUAGE_CODES if x.startswith(src_lang)]\n",
    "        assert len(match) == 1\n",
    "        lang = match[0]\n",
    "        tokenizer.src_lang = lang\n",
    "        dataset = read_mt_data(name=dataset)\n",
    "        dec_prefix = [tokenizer.eos_token_id,\n",
    "                      tokenizer.lang_code_to_id[\"en_XX\"]]\n",
    "        logging.info(f\"{tokenizer.decode(dec_prefix)}\")\n",
    "    model = model.to(device)\n",
    "    return tokenizer, model, dataset,dec_prefix\n",
    "\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "device = 'cuda:0'\n",
    "tokenizer, model, dataset, dec_prefix= setup_model(device_name=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def run_inference_step(model, input_ids, attention_mask=None, decoder_input_ids=None, targets=None, device='cuda:0', output_dec_hid=False, T=1):\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    if decoder_input_ids.size()[0] != input_ids.size()[0]:\n",
    "        target_batch_size = decoder_input_ids.size()[0]\n",
    "        batch_input_ids = input_ids.expand(target_batch_size, input_ids.size()[1])\n",
    "    else:\n",
    "        batch_input_ids = input_ids\n",
    "    assert decoder_input_ids.size()[0] == batch_input_ids.size()[0]\n",
    "\n",
    "    model_inputs = {\"input_ids\": batch_input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    }\n",
    "\n",
    "    outputs = model(**model_inputs,\n",
    "                    output_hidden_states=output_dec_hid,\n",
    "                    use_cache=False, return_dict=True)\n",
    "\n",
    "    # batch, dec seq, vocab size\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    if targets is not None:\n",
    "        targets = targets.to(device)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            input=next_token_logits, target=targets, reduction='none')\n",
    "    else:\n",
    "        loss = 0\n",
    "\n",
    "    prob = torch.nn.functional.softmax(next_token_logits/T, dim=-1)\n",
    "\n",
    "    return prob, next_token_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"BART model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).\"\n",
    "ref = \"BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\"\n",
    "doc_input_ids = torch.tensor(tokenizer(doc)['input_ids'],dtype=torch.long,device=device).unsqueeze(0)\n",
    "ref_ids_list = tokenizer(ref)['input_ids'][1:-1]\n",
    "ref_len = len(ref_ids_list)\n",
    "ref_input_ids = torch.tensor(ref_ids_list ,dtype=torch.long,device=device)\n",
    "\n",
    "print(doc_input_ids,ref_ids_list, ref_input_ids)\n",
    "\n",
    "dec_prefixes_id = [tokenizer.eos_token_id]\n",
    "\n",
    "\n",
    "for t in range(ref_len):\n",
    "    target  = ref_ids_list[t]\n",
    "    # print(dec_prefixes_id)\n",
    "    dec_input_tensors = torch.tensor(dec_prefixes_id, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    prob, next_token_logits, loss = run_inference_step(model=model, input_ids=doc_input_ids, decoder_input_ids=dec_input_tensors, device=device,)\n",
    "    oracle_prob = prob[0][target].tolist()\n",
    "    dec_prefixes_id.append(target)\n",
    "    print(oracle_prob)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = Network()\n",
    "g.add_nodes([1, 2, 3],\n",
    "            value=[1, 1, 1],\n",
    "            title=[\"I am node 1\", \"node 2 here\", \"and im node 3\"],\n",
    "            x=[0, 100, 100],\n",
    "            y=[0, 100, 200], label=[\"NODE 1\", \"NODE 2\", \"NODE 3\"],\n",
    "            color=[\"#00ff1e\", \"#162347\", \"#dd4b39\"])\n",
    "\n",
    "g.show('just_nodes.html')\n",
    "\n",
    "g.add_edge(1, 2)\n",
    "g.add_edge(1, 3)\n",
    "\n",
    "g.show('with_edges.html')\n",
    "\n",
    "for n in g.nodes:\n",
    "    n.update({'physics': False})\n",
    "\n",
    "g.show('example.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "sent_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Story Cloze Test is a new commonsense reasoning framework for evaluating story understanding, story generation, and script learning. I kinda hate it.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = sent_model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "sent_model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            \"/export/home/experimental/neurologic_decoding/gpt2-large/checkpoint-1800\",\n",
    "        )\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(290)\n",
      " and\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(\"A god leaps to his feet\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "logits = outputs.logits\n",
    "\n",
    "last = logits[0,-1,:]\n",
    "x = torch.argmax(last)\n",
    "print (x)\n",
    "print(tokenizer.decode(x.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67c54bf5fb630355f694c741165b3bdb09e3b950d114a736855d00c72ee312e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
